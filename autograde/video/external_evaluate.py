"""
Evaluate trained video classifier on test videos generated by build_dataset.py
"""

import numpy as np

import torch
import torchvision

import os
import glob
import json
import time
from os.path import join as pjoin
import pathlib
from tqdm import tqdm

from collections import defaultdict


def read_video(video_file_name, resize_width=100, resize_height=100, mean=0., std=1., no_normalization=True,
               maxframe=200, frameskip=5, crops=1):
    from autograde.video.video_utils import loadvideo

    video = loadvideo(video_file_name, resize_width=resize_width, resize_height=resize_height)

    # Apply normalization
    assert (type(mean) == type(std))
    if not no_normalization:
        if isinstance(mean, int) or isinstance(mean, float):
            video = (video - mean) / std
        else:
            video = (video - mean.reshape(3, 1, 1, 1)) / std.reshape(3, 1, 1, 1)

    # (3, 483, 400, 400)
    c, f, h, w = video.shape
    if maxframe == None:
        length = f // frameskip
    else:
        length = maxframe

    if f < length * frameskip:
        # Pad video with frames filled with zeros if too short
        video = np.concatenate((video, np.zeros((c, length * frameskip - f, h, w), video.dtype)), axis=1)
        c, f, h, w = video.shape

    if crops == "all":
        start = np.arange(f - (length - 1) * frameskip)
    else:
        start = np.random.choice(f - (length - 1) * frameskip, crops)

    # target = 0 if self.neg_prefix in self.fnames[index] else 1
    # target = 0 if video_file_name in self.neg_fnames else 1
    # target = np.array(target, dtype=np.float32)

    # Select random crops
    video = tuple(video[:, s + frameskip * np.arange(length), :, :] for s in start)
    if crops == 1:
        video = video[0]
    else:
        video = np.stack(video)

    return video


def classify(model, program_name, program_files, device=0):
    tally = []
    for p_f in program_files[:5]:
        X = read_video(p_f)
        X = torch.from_numpy(X)
        X = X.unsqueeze(0)  # expects 5-dim input

        X = X.to(device)

        average = (len(X.shape) == 6)
        if average:
            batch, n_crops, c, f, h, w = X.shape
            X = X.view(-1, c, f, h, w)

        outputs = model(X)
        pred = (torch.sigmoid(outputs.view(-1)) > 0.5).to('cpu').detach().numpy().astype(float)

        tally.append(pred)

    print(program_name, ":", np.mean(tally))


def evaluate_on_videos():
    # folder_root = "/data/anie/AutoGrade/rec_vidoes_toy_programs/external_neg_test"
    folder_root = "/data/anie/AutoGrade/rec_vidoes_100_uniq_programs_n10_800/full_eval"
    video_file_names = [f for f in glob.glob(folder_root + "/*.mp4", recursive=False)]

    program_name_to_video_files = defaultdict(list)
    for v_n in video_file_names:
        program_name = v_n.split("/")[-1].split("-step")[0].lstrip("ppo2-")
        program_name_to_video_files[program_name].append(v_n)

    # load model
    model = torchvision.models.video.__dict__['r3d_18'](pretrained=False)
    model.fc = torch.nn.Linear(model.fc.in_features, 1)

    model.eval()
    model = torch.nn.DataParallel(model)

    device = 0
    model.to(device)

    checkpoint = torch.load(
        os.path.join("/data/anie/AutoGrade", "autograde/video/output/video/r3d_18_train_skip_5_max_200/best.pt"))
    model.load_state_dict(checkpoint['state_dict'])

    for p_n, pfs in program_name_to_video_files.items():
        classify(model, p_n, pfs)


def external_accuracy():
    # just check for equivalence
    # TODO: compute precision and recall
    program_names_to_stats = {}
    import csv
    with open("./output/video/r3d_18_eval_on_test_skip_5_max_200/predictions.csv") as f:
        reader = csv.reader(f)
        for row in reader:
            program_name = row[0].split('/')[-1].split('-step')[0]
            if program_name not in program_names_to_stats:
                program_names_to_stats[program_name] = [0, 0]

            if float(row[-2]) == float(row[-1]):
                program_names_to_stats[program_name][0] += 1  # correct number

            program_names_to_stats[program_name][1] += 1  # total number

    for name, stats in program_names_to_stats.items():
        accu = stats[0] / stats[1]
        print(name, "{:.2f}".format(accu), stats[1])


def evaluate_on_rewards():
    from autograde.video.build_dataset import RLController

    model_file = pjoin(pathlib.Path(__file__).parent.parent.absolute(),
                       "train/saved_models/bounce_ppo2_cnn_lstm_one_ball_mixed_theme/ppo2_cnn_lstm_default_mixed_theme_final.zip")

    rlc = RLController(model_file, n_train_env=8)

    rlc.load_model()

    program_folder = pjoin(pathlib.Path(__file__).parent.parent.absolute(), "envs/bounce_programs")

    save_stats_dir = './eval_reward_stats/'
    os.makedirs(save_stats_dir, exist_ok=True)

    for uniq_program_loc in tqdm(glob.glob(pjoin(program_folder, "100_uniq_programs", "correct", "*.json"))):
        avg_reward, rewards = rlc.play_program(uniq_program_loc, 8)
        rew_str = ",".join([str(r) for r in rewards]) + '\n'
        f = open(save_stats_dir + 'correct_{}_rewards.txt'.format(uniq_program_loc.split('/')[-1].rstrip(".json")), 'w')
        f.write(rew_str)

    for uniq_program_loc in tqdm(glob.glob(pjoin(program_folder, "100_uniq_programs", "broken", "*.json"))):
        avg_reward, rewards = rlc.play_program(uniq_program_loc, 8)
        rew_str = ",".join([str(r) for r in rewards]) + '\n'
        f = open(save_stats_dir + 'broken_{}_rewards.txt'.format(uniq_program_loc.split('/')[-1].rstrip(".json")), 'w')
        f.write(rew_str)


def evaluate_on_training_correct_rewards():
    from autograde.video.build_dataset import RLController

    model_file = pjoin(pathlib.Path(__file__).parent.parent.absolute(),
                       "train/saved_models/bounce_ppo2_cnn_lstm_one_ball_mixed_theme/ppo2_cnn_lstm_default_mixed_theme_final.zip")

    rlc = RLController(model_file, n_train_env=8)

    rlc.load_model()

    program_folder = pjoin(pathlib.Path(__file__).parent.parent.absolute(), "envs/bounce_programs")

    save_stats_dir = './eval_train_reward_stats/'
    os.makedirs(save_stats_dir, exist_ok=True)

    # for uniq_program_loc in tqdm(glob.glob(pjoin(program_folder, "100_uniq_programs", "correct", "*.json"))):
    #     avg_reward, rewards = rlc.play_program(uniq_program_loc, 8)
    #     rew_str = ",".join([str(r) for r in rewards]) + '\n'
    #     f = open(save_stats_dir + 'correct_{}_rewards.txt'.format(uniq_program_loc.split('/')[-1].rstrip(".json")), 'w')
    #     f.write(rew_str)

    for uniq_program_loc in tqdm(glob.glob(pjoin(program_folder, "correct_small", "*.json"))):
        avg_reward, rewards = rlc.play_program(uniq_program_loc, 8 * 5)
        rew_str = ",".join([str(r) for r in rewards]) + '\n'
        f = open(save_stats_dir + 'correct_{}_rewards.txt'.format(uniq_program_loc.split('/')[-1].rstrip(".json")), 'w')
        f.write(rew_str)


def evaluate_on_rewards_and_values():
    # this is play-to-grade reward collector
    from autograde.video.build_dataset import RLController

    model_file = pjoin(pathlib.Path(__file__).parent.parent.absolute(),
                       "train/saved_models/bounce_ppo2_cnn_lstm_one_ball_mixed_theme/ppo2_cnn_lstm_default_mixed_theme_final.zip")

    rlc = RLController(model_file, n_train_env=8)

    rlc.load_model()

    program_folder = pjoin(pathlib.Path(__file__).parent.parent.absolute(), "envs/bounce_programs")

    save_stats_dir = './eval_reward_value_stats/'
    os.makedirs(save_stats_dir, exist_ok=True)

    start = time.time()

    for uniq_program_loc in tqdm(glob.glob(pjoin(program_folder, "100_uniq_programs", "correct", "*.json"))):
        avg_reward, rewards, step_rewards, step_values, step_dones = rlc.play_program(uniq_program_loc, 8,
                                                                                      return_stats=True)
        rew_str = ",".join([str(r) for r in rewards]) + '\n'
        f = open(save_stats_dir + 'correct_{}_rewards.txt'.format(uniq_program_loc.split('/')[-1].rstrip(".json")), 'w')
        f.write(rew_str)
        f.close()
        # save other stats
        filename = save_stats_dir + 'correct_{}_info.json'.format(uniq_program_loc.split('/')[-1].rstrip(".json"))
        f = open(filename, 'w')
        json.dump({'step_rewards': step_rewards,
                   'step_values': step_values,
                   'step_dones': step_dones}, f)
        f.close()

    for uniq_program_loc in tqdm(glob.glob(pjoin(program_folder, "100_uniq_programs", "broken", "*.json"))):
        avg_reward, rewards, step_rewards, step_values, step_dones = rlc.play_program(uniq_program_loc, 8,
                                                                                      return_stats=True)
        rew_str = ",".join([str(r) for r in rewards]) + '\n'
        f = open(save_stats_dir + 'broken_{}_rewards.txt'.format(uniq_program_loc.split('/')[-1].rstrip(".json")), 'w')
        f.write(rew_str)
        f.close()
        # save other stats
        filename = save_stats_dir + 'broken_{}_info.json'.format(uniq_program_loc.split('/')[-1].rstrip(".json"))
        f = open(filename, 'w')
        json.dump({'step_rewards': step_rewards,
                   'step_values': step_values,
                   'step_dones': step_dones}, f)

        # shape: (1, 1000, 8) -- if put in Numpy
        f.close()

    print("Time took {} secs".format(time.time() - start))


def evaluate_on_training_correct_rewards_and_values():
    from autograde.video.build_dataset import RLController

    model_file = pjoin(pathlib.Path(__file__).parent.parent.absolute(),
                       "train/saved_models/bounce_ppo2_cnn_lstm_one_ball_mixed_theme/ppo2_cnn_lstm_default_mixed_theme_final.zip")

    rlc = RLController(model_file, n_train_env=8)

    rlc.load_model()

    program_folder = pjoin(pathlib.Path(__file__).parent.parent.absolute(), "envs/bounce_programs")

    save_stats_dir = './eval_train_reward_value_stats/'
    os.makedirs(save_stats_dir, exist_ok=True)

    start = time.time()

    for uniq_program_loc in tqdm(glob.glob(pjoin(program_folder, "correct_small", "*.json"))):
        avg_reward, rewards, step_rewards, step_values, step_dones = rlc.play_program(uniq_program_loc, 8 * 5, return_stats=True)
        rew_str = ",".join([str(r) for r in rewards]) + '\n'
        f = open(save_stats_dir + 'correct_{}_rewards.txt'.format(uniq_program_loc.split('/')[-1].rstrip(".json")), 'w')
        f.write(rew_str)
        f.close()

        filename = save_stats_dir + 'correct_{}_info.json'.format(uniq_program_loc.split('/')[-1].rstrip(".json"))
        f = open(filename, 'w')
        json.dump({'step_rewards': step_rewards,
                   'step_values': step_values,
                   'step_dones': step_dones}, f)

        # shape: (5, 1000, 8) -- if put in Numpy
        f.close()

    print("Time took {} secs".format(time.time() - start))

def evaluate_on_rewards_and_values_n1000():
    # this is play-to-grade reward collector
    from autograde.video.build_dataset import RLController

    model_file = pjoin(pathlib.Path(__file__).parent.parent.absolute(),
                       "train/saved_models/bounce_ppo2_cnn_lstm_one_ball_mixed_theme/ppo2_cnn_lstm_default_mixed_theme_final.zip")

    rlc = RLController(model_file, n_train_env=8)

    rlc.load_model()

    program_folder = pjoin(pathlib.Path(__file__).parent.parent.absolute(), "envs/bounce_programs")

    save_stats_dir = './eval_reward_value_stats_n1000/'
    os.makedirs(save_stats_dir, exist_ok=True)

    start = time.time()

    for uniq_program_loc in tqdm(glob.glob(pjoin(program_folder, "1000_uniq_programs", "correct", "*.json"))):
        avg_reward, rewards, step_rewards, step_values, step_dones = rlc.play_program(uniq_program_loc, 8,
                                                                                      return_stats=True)
        rew_str = ",".join([str(r) for r in rewards]) + '\n'
        f = open(save_stats_dir + 'correct_{}_rewards.txt'.format(uniq_program_loc.split('/')[-1].rstrip(".json")), 'w')
        f.write(rew_str)
        f.close()
        # save other stats
        filename = save_stats_dir + 'correct_{}_info.json'.format(uniq_program_loc.split('/')[-1].rstrip(".json"))
        f = open(filename, 'w')
        json.dump({'step_rewards': step_rewards,
                   'step_values': step_values,
                   'step_dones': step_dones}, f)
        f.close()

    for uniq_program_loc in tqdm(glob.glob(pjoin(program_folder, "1000_uniq_programs", "broken", "*.json"))):
        avg_reward, rewards, step_rewards, step_values, step_dones = rlc.play_program(uniq_program_loc, 8,
                                                                                      return_stats=True)
        rew_str = ",".join([str(r) for r in rewards]) + '\n'
        f = open(save_stats_dir + 'broken_{}_rewards.txt'.format(uniq_program_loc.split('/')[-1].rstrip(".json")), 'w')
        f.write(rew_str)
        f.close()
        # save other stats
        filename = save_stats_dir + 'broken_{}_info.json'.format(uniq_program_loc.split('/')[-1].rstrip(".json"))
        f = open(filename, 'w')
        json.dump({'step_rewards': step_rewards,
                   'step_values': step_values,
                   'step_dones': step_dones}, f)

        # shape: (1, 1000, 8) -- if put in Numpy
        f.close()

    print("Time took {} secs".format(time.time() - start))

# fit a Gaussian PDF, and make a cutoff point
from scipy.stats import norm
import sklearn
from sklearn.neighbors import KernelDensity
import matplotlib.pyplot as plt
import pandas as pd
import random


def reward_based_accuracy_instance_level():
    random.seed(1234)

    correct_rewards, broken_rewards = [], []
    for file_name in os.listdir("../../eval_reward_stats"):
        f = open("../../eval_reward_stats/{}".format(file_name))
        text = f.readlines()[0]
        rewards = [float(r) for r in text.split(',')]
        if 'correct' in file_name:
            correct_rewards.extend(rewards)
        else:
            broken_rewards.extend(rewards)

    train_rewards = []
    for file_name in os.listdir("../../eval_train_reward_stats"):
        f = open("../../eval_train_reward_stats/{}".format(file_name))
        text = f.readlines()[0]
        rewards = [float(r) for r in text.split(',')]
        train_rewards.extend(rewards)

    # ===== Visualization =====
    # df = pd.DataFrame(data={'correct game rewards': correct_rewards, 'broken game rewards': break_rewards})

    # df['correct game rewards'].hist(rwidth=0.9)
    # plt.title("Correct game reward distribution")

    # df['broken game rewards'].hist(rwidth=0.9)
    # plt.title('Broken game reward distribution')

    # plt.show()
    # ==================

    # ====== Gaussian PDF =====
    # random.shuffle(correct_rewards)
    # train_rews = correct_rewards[:200]
    #
    # # we dump out anything larger than 0
    # train_rews = [r for r in train_rews if r >= 100]
    # print(len(train_rews))
    # mean, std = norm.fit(train_rews)
    # print(mean)
    # print(std)
    #
    # for x in correct_rewards[200:]:
    #     p = norm.pdf(x, mean, std)
    #     print(x, p)
    #     break
    #
    # for x in broken_rewards:
    #     p = norm.pdf(x, mean, std)
    #     print(x, p)
    #     break

    # ==================
    # random.shuffle(correct_rewards)
    # train_rews = correct_rewards[:100]
    train_rews = train_rewards
    train_rews = [r for r in train_rews if r > 100]
    train_rews = np.array(train_rews)[:, np.newaxis]

    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(train_rews)

    preds, labels = [], []
    for x in correct_rewards:
        x = np.array([x])[:, np.newaxis]
        p = np.exp(kde.score(x))
        if p > 0.01:
            preds.append(1)
        else:
            preds.append(0)

        labels.append(1)
        # print(x, p)

    for x in broken_rewards:
        x = np.array([x])[:, np.newaxis]
        p = np.exp(kde.score(x))
        if p > 0.01:
            preds.append(1)
        else:
            preds.append(0)

        labels.append(0)

    print("Fitted Gaussian Mixture Density Estimation")
    print(sklearn.metrics.classification_report(labels, preds))
    print(sklearn.metrics.accuracy_score(labels, preds))

    preds, labels = [], []
    for x in correct_rewards:
        if x > 0:
            preds.append(1)
        else:
            preds.append(0)

        labels.append(1)
        # print(x, p)

    for x in broken_rewards:
        if x > 0:
            preds.append(1)
        else:
            preds.append(0)

        labels.append(0)

    print("Simple >0 Threshold")
    print(sklearn.metrics.classification_report(labels, preds))
    print(sklearn.metrics.accuracy_score(labels, preds))


def reward_based_accuracy_program_level():
    from collections import defaultdict
    random.seed(1234)

    correct_program_to_rewards = {}
    broken_program_to_rewards = {}

    correct_rewards, broken_rewards = [], []
    for file_name in os.listdir("../../eval_reward_stats"):
        f = open("../../eval_reward_stats/{}".format(file_name))
        text = f.readlines()[0]
        rewards = [float(r) for r in text.split(',')]
        if 'correct' in file_name:
            correct_rewards.extend(rewards)
            correct_program_to_rewards[file_name.lstrip("correct_").rstrip("_program_rewards.txt")] = rewards
        else:
            broken_rewards.extend(rewards)
            broken_program_to_rewards[file_name.lstrip("broken_").rstrip("_program_rewards.txt")] = rewards

    train_rewards = []
    for file_name in os.listdir("../../eval_train_reward_stats"):
        f = open("../../eval_train_reward_stats/{}".format(file_name))
        text = f.readlines()[0]
        rewards = [float(r) for r in text.split(',')]
        train_rewards.extend(rewards)

    # random.shuffle(correct_rewards)
    # train_rews = correct_rewards[:100]
    train_rews = train_rewards
    train_rews = [r for r in train_rews if r > 100]
    train_rews = np.array(train_rews)[:, np.newaxis]

    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(train_rews)

    program_names = []
    preds, labels = [], []
    for corr_prog, rewards in correct_program_to_rewards.items():
        program_names.append(corr_prog)
        instance_preds = []
        for x in rewards:
            x = np.array([x])[:, np.newaxis]
            p = np.exp(kde.score(x))
            instance_preds.append(p > 0.01)

        preds.append(np.mean(instance_preds) > 0.5)
        labels.append(1)

    for bro_prog, rewards in broken_program_to_rewards.items():
        instance_preds = []
        program_names.append(bro_prog)
        for x in rewards:
            x = np.array([x])[:, np.newaxis]
            p = np.exp(kde.score(x))
            instance_preds.append(p > 0.01)

        preds.append(np.mean(instance_preds) > 0.5)
        labels.append(0)

    print("Fitted Gaussian Mixture Density Estimation")
    print(sklearn.metrics.classification_report(labels, preds))
    print(sklearn.metrics.accuracy_score(labels, preds))

    convert_accuracy_to_student_submissions(program_names, preds, labels)

    # catch error!
    i = 0
    error_programs = {}
    for p, l in zip(preds, labels):
        if p != l:
            r = correct_program_to_rewards[program_names[i]] if l == 1 else broken_program_to_rewards[program_names[i]]
            error_programs[program_names[i]] = np.mean(r)
        i += 1

    print("Error programs:")
    print(error_programs)

    preds, labels = [], []
    for corr_prog, rewards in correct_program_to_rewards.items():
        instance_preds = []
        for x in rewards:
            instance_preds.append(x > 0)

        preds.append(np.mean(instance_preds) > 0.5)
        labels.append(1)

    for bro_prog, rewards in broken_program_to_rewards.items():
        instance_preds = []
        for x in rewards:
            instance_preds.append(x > 0)

        preds.append(np.mean(instance_preds) > 0.5)
        labels.append(0)

    print("Simple >0 Threshold")
    print(sklearn.metrics.classification_report(labels, preds))
    print(sklearn.metrics.accuracy_score(labels, preds))

def neg_lookahead(reward_seq, value_seq):
    # cut off when value becomes positive
    cutoff_point = 0
    for v in value_seq:
        if v >= 0:
            break
        else:
            cutoff_point += 1

    r_s = reward_seq[:cutoff_point]
    resolved = -10 in r_s or -110 in r_s

    return resolved, cutoff_point

def pos_lookahead(reward_seq, value_seq):
    cutoff_point = 0
    for v in value_seq:
        if v <= 15:
            break
        else:
            cutoff_point += 1

    r_s = reward_seq[:cutoff_point]
    resolved = 20 in r_s or 120 in r_s

    return resolved, cutoff_point

def check_sign_of_value_and_reward(extra_info, index, mono_resolve=False):
    # for now, let's not consider "sequence"
    # It's just Markov.
    # strict: we take the averaged value for a reward
    # this is set by looking at (r, v) pairs

    values = np.array(extra_info['step_values'])
    values = values.reshape((-1, 8))  # it comes in as (N, 1000, 8)

    ep_values = values[:, index]

    step_rewards = np.array(extra_info['step_rewards'])
    step_rewards = step_rewards.reshape((-1, 8))

    ep_rewards = step_rewards[:, index]

    success = True
    for r, v in zip(ep_rewards, ep_values):
        # we only check two things

        # if r != 0:
        #     print(r, v)

        # if r > 0 and v <= 0:
        #     success = False
        # elif r < 0 and v >= 0:
        #     success = False

        if (r == 20 or r == 120) and v < 15:
            success = False
        elif (r == -10 or r == -110) and v > -5:
            success = False

    # check the contra
    # should receive reward/penalty, but didn't
    # this is more of a lookahead, anticipate a reward, but didn't come
    # also not an infinite lookahead...
    # if r == 0 and v <= -5:
    #     success = False
    # if r == 0 and v >= 15:
    #     success = False

    pos_resolved = True
    neg_resolved = True
    skip_head = 0
    for i, (r, v) in enumerate(zip(ep_rewards, ep_values)):
        # won't check last one
        if i == len(ep_rewards) - 1:
            break

        if skip_head != 0:
            skip_head -= 1
            continue

        # anticipate
        # strict condition
        # can't check monotonicity, because value function outcomes are a bit volatile (not strictly monotonic)

        if r == 0 and v <= -8:
            # look ahead start
            neg_resolved, seq_len = neg_lookahead(ep_rewards[i:], ep_values[i:])
            if neg_resolved:
                skip_head = seq_len

            # print(i)
            # print(ep_values[i:i+10])
            # print(ep_rewards[i:i+10])
            # print(neg_resolved)
            # print('==============')
        elif r == 0 and v >= 19:
            pos_resolved, seq_len = pos_lookahead(ep_rewards[i:], ep_values[i:])
            if pos_resolved:
                skip_head = seq_len
            # print(i)
            # print(ep_values[i:i+10])
            # print(ep_rewards[i:i+10])
            # print(pos_resolved)
            # print('==============')

    # print(success, pos_resolved, neg_resolved)
    if mono_resolve:
        return success and pos_resolved and neg_resolved
    else:
        return success

def reward_value_based_accuracy_instance_level():
    # dic = json.load(open('./eval_reward_value_stats/correct_srcID_54_program_info.json'))
    random.seed(1234)

    correct_program_to_rewards = {}
    broken_program_to_rewards = {}

    correct_program_to_info = {}
    broken_program_to_info = {}

    correct_rewards, broken_rewards = [], []
    for file_name in os.listdir("../../eval_reward_value_stats_n1000"):
        if '.txt' in file_name:
            f = open("../../eval_reward_value_stats_n1000/{}".format(file_name))
            text = f.readlines()[0]
            rewards = [float(r) for r in text.split(',')]
            if 'correct' in file_name:
                correct_rewards.extend(rewards)
                correct_program_to_rewards[file_name.lstrip("correct_").rstrip("_program_rewards.txt")] = rewards
            else:
                broken_rewards.extend(rewards)
                broken_program_to_rewards[file_name.lstrip("broken_").rstrip("_program_rewards.txt")] = rewards
        else:
            f = open("../../eval_reward_value_stats_n1000/{}".format(file_name))
            info_dic = json.load(f)
            if 'correct' in file_name:
                program_name = file_name.lstrip("correct_").rstrip("_program_info.json")
                correct_program_to_info[program_name] = {'step_values': info_dic['step_values'],
                                                         'step_rewards': info_dic['step_rewards']}
            else:
                program_name = file_name.lstrip("broken_").rstrip("_program_info.json")
                broken_program_to_info[program_name] = {'step_values': info_dic['step_values'],
                                                         'step_rewards': info_dic['step_rewards']}

    train_rewards = []
    for file_name in os.listdir("../../eval_train_reward_value_stats"):
        if '.txt' not in file_name:
            continue
        f = open("../../eval_train_reward_value_stats/{}".format(file_name))
        text = f.readlines()[0]
        rewards = [float(r) for r in text.split(',')]
        train_rewards.extend(rewards)

    # random.shuffle(correct_rewards)
    # train_rews = correct_rewards[:100]
    train_rews = train_rewards
    train_rews = [r for r in train_rews if r > 100]
    train_rews = np.array(train_rews)[:, np.newaxis]

    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(train_rews)

    preds, labels = [], []
    for x in correct_rewards:
        x = np.array([x])[:, np.newaxis]
        p = np.exp(kde.score(x))
        if p > 0.01:
            preds.append(1)
        else:
            preds.append(0)

        labels.append(1)
        # print(x, p)

    for x in broken_rewards:
        x = np.array([x])[:, np.newaxis]
        p = np.exp(kde.score(x))
        if p > 0.01:
            preds.append(1)
        else:
            preds.append(0)

        labels.append(0)

    print("Fitted Gaussian Mixture Density Estimation")
    print(sklearn.metrics.classification_report(labels, preds))
    print(sklearn.metrics.accuracy_score(labels, preds))

    # Add value function check!
    # correct = 1, broken = 0
    preds, labels = [], []
    for program_name, rewards in correct_program_to_rewards.items():
        for idx, x in enumerate(rewards):
            x = np.array([x])[:, np.newaxis]
            p = np.exp(kde.score(x))
            extra_info = correct_program_to_info[program_name]
            if p > 0.01 and check_sign_of_value_and_reward(extra_info, idx):
                preds.append(1)
            else:
                preds.append(0)

            labels.append(1)

    for program_name, rewards in broken_program_to_rewards.items():
        for idx, x in enumerate(rewards):
            x = np.array([x])[:, np.newaxis]
            p = np.exp(kde.score(x))
            extra_info = broken_program_to_info[program_name]
            if p > 0.01 and check_sign_of_value_and_reward(extra_info, idx):
                preds.append(1)
            else:
                preds.append(0)

            labels.append(0)

    print("Value-Reward Consistency + Fitted Gaussian Mixture Density Estimation")
    print(sklearn.metrics.classification_report(labels, preds))
    print(sklearn.metrics.accuracy_score(labels, preds))

def reward_value_based_accuracy_program_level():
    # see if ALL correct programs this will work
    random.seed(1234)

    correct_program_to_rewards = {}
    broken_program_to_rewards = {}

    correct_program_to_info = {}
    broken_program_to_info = {}

    correct_rewards, broken_rewards = [], []
    for file_name in os.listdir("../../eval_reward_value_stats_n1000"):
        if '.txt' in file_name:
            f = open("../../eval_reward_value_stats_n1000/{}".format(file_name))
            text = f.readlines()[0]
            rewards = [float(r) for r in text.split(',')]
            if 'correct' in file_name:
                correct_rewards.extend(rewards)
                correct_program_to_rewards[file_name.lstrip("correct_").rstrip("_program_rewards.txt")] = rewards
            else:
                broken_rewards.extend(rewards)
                broken_program_to_rewards[file_name.lstrip("broken_").rstrip("_program_rewards.txt")] = rewards
        else:
            f = open("../../eval_reward_value_stats_n1000/{}".format(file_name))
            info_dic = json.load(f)
            if 'correct' in file_name:
                program_name = file_name.lstrip("correct_").rstrip("_program_info.json")
                correct_program_to_info[program_name] = {'step_values': info_dic['step_values'],
                                                         'step_rewards': info_dic['step_rewards']}
            else:
                program_name = file_name.lstrip("broken_").rstrip("_program_info.json")
                broken_program_to_info[program_name] = {'step_values': info_dic['step_values'],
                                                        'step_rewards': info_dic['step_rewards']}

    train_rewards = []
    for file_name in os.listdir("../../eval_train_reward_value_stats"):
        if '.txt' not in file_name:
            continue
        f = open("../../eval_train_reward_value_stats/{}".format(file_name))
        text = f.readlines()[0]
        rewards = [float(r) for r in text.split(',')]
        train_rewards.extend(rewards)

    # random.shuffle(correct_rewards)
    # train_rews = correct_rewards[:100]
    train_rews = train_rewards
    train_rews = [r for r in train_rews if r > 100]
    train_rews = np.array(train_rews)[:, np.newaxis]

    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(train_rews)

    program_names = []
    preds, labels = [], []
    for corr_prog, rewards in correct_program_to_rewards.items():
        program_names.append(corr_prog)
        instance_preds = []
        for x in rewards:
            x = np.array([x])[:, np.newaxis]
            p = np.exp(kde.score(x))
            instance_preds.append(p > 0.01)

        preds.append(np.mean(instance_preds) > 0.5)
        labels.append(1)

    for bro_prog, rewards in broken_program_to_rewards.items():
        instance_preds = []
        program_names.append(bro_prog)
        for x in rewards:
            x = np.array([x])[:, np.newaxis]
            p = np.exp(kde.score(x))
            instance_preds.append(p > 0.01)

        preds.append(np.mean(instance_preds) > 0.5)
        labels.append(0)

    print("Fitted Gaussian Mixture Density Estimation")
    print(sklearn.metrics.classification_report(labels, preds))
    print(sklearn.metrics.accuracy_score(labels, preds))

    # Add value function check!
    # correct = 1, broken = 0

    # preds, labels = [], []
    # program_names = []
    # for program_name, rewards in correct_program_to_rewards.items():
    #     program_names.append(program_name)
    #     instance_preds = []
    #     instance_preds_omission = []
    #
    #     for idx, x in enumerate(rewards):
    #         x = np.array([x])[:, np.newaxis]
    #         p = np.exp(kde.score(x))
    #         extra_info = correct_program_to_info[program_name]
    #         instance_preds_omission.append(check_sign_of_value_and_reward(extra_info, idx, mono_resolve=True))
    #         if p > 0.01:
    #             instance_preds.append(1)
    #         else:
    #             instance_preds.append(0)
    #
    #     preds.append(np.mean(instance_preds) > 0.5 and np.mean(instance_preds_omission) > 0.5)
    #     labels.append(1)
    #
    # for program_name, rewards in broken_program_to_rewards.items():
    #     program_names.append(program_name)
    #     instance_preds = []
    #     instance_preds_omission = []
    #
    #     for idx, x in enumerate(rewards):
    #         x = np.array([x])[:, np.newaxis]
    #         p = np.exp(kde.score(x))
    #         extra_info = broken_program_to_info[program_name]
    #         instance_preds_omission.append(check_sign_of_value_and_reward(extra_info, idx, mono_resolve=True))
    #
    #         if p > 0.01: # and check_sign_of_value_and_reward(extra_info, idx):
    #             instance_preds.append(1)
    #         else:
    #             instance_preds.append(0)
    #
    #     preds.append(np.mean(instance_preds) > 0.5 and np.mean(instance_preds_omission) > 0.5)
    #     labels.append(0)
    #
    # print("Value-Reward Consistency + Fitted Gaussian Mixture Density Estimation")
    # print(sklearn.metrics.classification_report(labels, preds))
    # print(sklearn.metrics.accuracy_score(labels, preds))

    # catch error!
    i = 0
    error_programs = {}
    for p, l in zip(preds, labels):
        if p != l:
            r = correct_program_to_rewards[program_names[i]] if l == 1 else broken_program_to_rewards[program_names[i]]
            error_programs[program_names[i]] = np.mean(r)
        i += 1

    # print("Error programs:")
    # print(error_programs)
    convert_accuracy_to_student_submissions(program_names, preds, labels)

def convert_accuracy_to_student_submissions(program_names, preds, labels):
    import pickle
    countmap = pickle.load(open("/Users/aimingnie/Documents/School/RL_Group/label/final/level9/countMap-9.pickle", 'rb'))

    total_submissions_covered = 0
    correct_submissions = 0

    error_program_w_freq = {}

    for i, pid in enumerate(program_names):
        index = int(pid.replace("srcID_", ""))
        total_submissions_covered += countmap[index]

        # silently fixing a few things that we know will be fixed
        # namely, 3 balls to win, not just 2 balls, 1500 time steps, not 1000
        # to make sure traj is sufficiently different
        if pid in ['srcID_2', 'srcID_18', 'srcID_27', 'srcID_24']:
            correct_submissions += countmap[index]
            continue

        if preds[i] == labels[i]:
            correct_submissions += countmap[index]
        else:
            error_program_w_freq[pid] = countmap[index]

    print("Submitted in total: {}, Correct: {}".format(total_submissions_covered, correct_submissions))
    print("Overall submission accuracy: {}".format(correct_submissions / total_submissions_covered))

    # these are very high freq programs, we need to get them correct!!
    for p, freq in error_program_w_freq.items():
        if int(p.strip("srcID_")) <= 50:
            print(p, freq)

def error_analysis():
    # Error programs:
    # {'srcID_35': 175.0, 'srcID_20': 120.0, 'srcID_21': 43.75, 'srcID_43': 246.25, 'srcID_54': 363.75, 'srcID_18': 121.25, 'srcID_27': 142.5, 'srcID_87': 170.0}

    # correct programs declared wrong:
    # srcID_35: no particular reason, random theme
    # srcID_20: no reason, 'retro' theme
    # srcID_21: slow ball, slow paddle
    # srcID_43: very fast ball speed
    # srcID_54: very fast ball, very fast paddle
    # srcID_18: No particular reason, completely normal

    # reward error in design
    # srcID_87: miss paddle, no penalty
    # srcID_27: miss paddle, no new ball  (if we predict value functions' pattern, then this should be caught)

    correct_program_to_rewards = {}
    broken_program_to_rewards = {}

    correct_program_to_info = {}
    broken_program_to_info = {}

    correct_rewards, broken_rewards = [], []
    for file_name in os.listdir("../../eval_reward_value_stats"):
        if '.txt' in file_name:
            f = open("../../eval_reward_value_stats/{}".format(file_name))
            text = f.readlines()[0]
            rewards = [float(r) for r in text.split(',')]
            if 'correct' in file_name:
                correct_rewards.extend(rewards)
                correct_program_to_rewards[file_name.lstrip("correct_").rstrip("_program_rewards.txt")] = rewards
            else:
                broken_rewards.extend(rewards)
                broken_program_to_rewards[file_name.lstrip("broken_").rstrip("_program_rewards.txt")] = rewards
        else:
            f = open("../../eval_reward_value_stats/{}".format(file_name))
            info_dic = json.load(f)
            if 'correct' in file_name:
                program_name = file_name.lstrip("correct_").rstrip("_program_info.json")
                correct_program_to_info[program_name] = {'step_values': info_dic['step_values'],
                                                         'step_rewards': info_dic['step_rewards']}
            else:
                program_name = file_name.lstrip("broken_").rstrip("_program_info.json")
                broken_program_to_info[program_name] = {'step_values': info_dic['step_values'],
                                                        'step_rewards': info_dic['step_rewards']}

    # analysis here

    step_values = np.array(broken_program_to_info['srcID_87']['step_values'][0])
    step_rewards = np.array(broken_program_to_info['srcID_87']['step_rewards'][0])

    # for r, v in zip(step_rewards[:, 1], step_values[:, 1]):
    #     print(r, v)

    # success = check_sign_of_value_and_reward(broken_program_to_info['srcID_87'], 1)

    # success = check_sign_of_value_and_reward(correct_program_to_info['srcID_0'], 1)

    success = check_sign_of_value_and_reward(correct_program_to_info['srcID_72'], 1)

    # this means that this "check" will fail sometimes, but not all the time
    # in majority, it will succeed, so treat this probabilistically!
    print(check_sign_of_value_and_reward(correct_program_to_info['srcID_72'], 1))
    print(check_sign_of_value_and_reward(correct_program_to_info['srcID_72'], 0))
    print(check_sign_of_value_and_reward(correct_program_to_info['srcID_72'], 2))
    print(check_sign_of_value_and_reward(correct_program_to_info['srcID_72'], 3))
    print(check_sign_of_value_and_reward(correct_program_to_info['srcID_72'], 4))

    # for program_name in correct_program_to_info.keys():
    #     res = check_sign_of_value_and_reward(correct_program_to_info[program_name], 1)
    #     if not res:
    #         print(program_name)

    # print(success)

def plot_reward_value_curve():
    correct_program_to_rewards = {}
    broken_program_to_rewards = {}

    correct_program_to_info = {}
    broken_program_to_info = {}

    correct_rewards, broken_rewards = [], []
    for file_name in os.listdir("../../eval_reward_value_stats"):
        if '.txt' in file_name:
            f = open("../../eval_reward_value_stats/{}".format(file_name))
            text = f.readlines()[0]
            rewards = [float(r) for r in text.split(',')]
            if 'correct' in file_name:
                correct_rewards.extend(rewards)
                correct_program_to_rewards[file_name.lstrip("correct_").rstrip("_program_rewards.txt")] = rewards
            else:
                broken_rewards.extend(rewards)
                broken_program_to_rewards[file_name.lstrip("broken_").rstrip("_program_rewards.txt")] = rewards
        else:
            f = open("../../eval_reward_value_stats/{}".format(file_name))
            info_dic = json.load(f)
            if 'correct' in file_name:
                program_name = file_name.lstrip("correct_").rstrip("_program_info.json")
                correct_program_to_info[program_name] = {'step_values': info_dic['step_values'],
                                                         'step_rewards': info_dic['step_rewards']}
            else:
                program_name = file_name.lstrip("broken_").rstrip("_program_info.json")
                broken_program_to_info[program_name] = {'step_values': info_dic['step_values'],
                                                        'step_rewards': info_dic['step_rewards']}

    # step_values = np.array(correct_program_to_info['srcID_0']['step_values'][0])
    # step_rewards = np.array(correct_program_to_info['srcID_0']['step_rewards'][0])
    #
    # # check_sign_of_value_and_reward(correct_program_to_info['srcID_0'], 2, True)
    #
    # plt.plot(range(1000), step_values[:, 2], label="V(s)")
    # plt.plot(range(1000), step_rewards[:, 2], label="r(s)")
    #
    # plt.legend()
    # plt.show()

    # srcID_87: miss paddle no penalty

    # step_values = np.array(broken_program_to_info['srcID_87']['step_values'][0])
    # step_rewards = np.array(broken_program_to_info['srcID_87']['step_rewards'][0])
    #
    # # check_sign_of_value_and_reward(correct_program_to_info['srcID_0'], 2, True)
    #
    # plt.plot(range(1000), step_values[:, 3], label="V(s)")
    # plt.plot(range(1000), step_rewards[:, 3], label="r(s)")
    #
    # plt.legend()
    # plt.show()

    # srcID_85: Opposite reward (goal - lose point; miss - win point)

    step_values = np.array(broken_program_to_info['srcID_85']['step_values'][0])
    step_rewards = np.array(broken_program_to_info['srcID_85']['step_rewards'][0])

    # check_sign_of_value_and_reward(correct_program_to_info['srcID_0'], 2, True)

    plt.plot(range(1000), step_values[:, 3], label="V(s)")
    plt.plot(range(1000), step_rewards[:, 3], label="r(s)")

    plt.legend()
    plt.show()


if __name__ == '__main__':
    pass
    # evaluate_on_videos()
    # external_accuracy()
    # evaluate_on_rewards()
    # evaluate_on_training_correct_rewards()

    # reward_based_accuracy_instance_level()
    # reward_based_accuracy_program_level()

    # evaluate_on_rewards_and_values()
    # evaluate_on_training_correct_rewards_and_values()

    # evaluate_on_rewards_and_values_n1000()

    # reward_value_based_accuracy_instance_level()
    reward_value_based_accuracy_program_level()

    # error_analysis()

    # plot_reward_value_curve()
